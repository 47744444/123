{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZQ/Qlyo09HBQ70JFOCgNw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/47744444/123/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x5nkIDsNwNGv"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation, Conv2D, MaxPooling2D\n",
        "#下載所需檔案"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip train.zip\n",
        "#解壓縮"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1YsZWyVwfqq",
        "outputId": "a2a37da6-2fc5-4042-9d00-d79ca9f64235"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  train.zip\n",
            "   creating: ╖s╝W╕ъо╞зи/\n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.0.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.1.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.10.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.11.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.12.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.13.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.14.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.15.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.16.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.17.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.18.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.19.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.2.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.20.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.21.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.22.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.23.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.24.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.25.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.26.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.27.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.28.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.29.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.3.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.30.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.31.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.4.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.5.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.6.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.7.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.8.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/cat.9.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.0.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.1.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.10.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.11.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.12.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.13.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.14.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.15.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.16.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.17.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.18.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.19.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.2.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.20.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.21.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.22.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.23.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.24.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.25.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.26.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.27.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.28.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.29.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.3.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.30.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.4.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.5.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.6.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.7.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.8.jpg  \n",
            "  inflating: ╖s╝W╕ъо╞зи/dog.9.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip test1.zip\n",
        "#解壓縮"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYRob_elwjES",
        "outputId": "fbfd7bd6-5018-4943-feb0-fbf19007e208"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  test1.zip\n",
            "   creating: test1/\n",
            "  inflating: test1/cat.152.jpg       \n",
            "  inflating: test1/cat.153.jpg       \n",
            "  inflating: test1/cat.154.jpg       \n",
            "  inflating: test1/cat.155.jpg       \n",
            "  inflating: test1/cat.156.jpg       \n",
            "  inflating: test1/cat.157.jpg       \n",
            "  inflating: test1/cat.158.jpg       \n",
            "  inflating: test1/cat.159.jpg       \n",
            "  inflating: test1/dog.207.jpg       \n",
            "  inflating: test1/dog.208.jpg       \n",
            "  inflating: test1/dog.209.jpg       \n",
            "  inflating: test1/dog.210.jpg       \n",
            "  inflating: test1/dog.211.jpg       \n",
            "  inflating: test1/dog.212.jpg       \n",
            "  inflating: test1/dog.213.jpg       \n",
            "  inflating: test1/dog.214.jpg       \n",
            "  inflating: test1/dog.215.jpg       \n",
            "  inflating: test1/dog.216.jpg       \n",
            "  inflating: test1/dog.217.jpg       \n",
            "  inflating: test1/dog.218.jpg       \n",
            "  inflating: test1/dog.219.jpg       \n",
            "  inflating: test1/dog.220.jpg       \n",
            "  inflating: test1/dog.221.jpg       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"train\"\n",
        "path = os.path.join(train_dir)\n",
        "X = []\n",
        "y = []\n",
        "convert = lambda category : int(category == 'dog')\n",
        "def create_test_data(path):\n",
        "    for p in os.listdir(path):\n",
        "        category = p.split(\".\")[0]\n",
        "        category = convert(category)\n",
        "        img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n",
        "        new_img_array = cv2.resize(img_array, dsize=(80, 80))\n",
        "        X.append(new_img_array)\n",
        "        y.append(category)\n",
        "create_test_data(path)\n",
        "X = np.array(X).reshape(-1, 80,80,1)\n",
        "y = np.array(y)\n",
        "#固定辨識圖片大小\n",
        "X = X/255.0"
      ],
      "metadata": {
        "id": "XMZTmferwoqV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "# 捕抓特徵\n",
        "model.add(Conv2D(64,(3,3), activation = 'relu', input_shape = X.shape[1:]))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "# 添加繳活層函數\n",
        "model.add(Conv2D(64,(3,3), activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "# 加入最軟層的判定\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=\"adam\",loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "2r7goJ17w1FJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=15, batch_size=32)\n",
        "#訓練樣本"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "079LKbs9w-Gq",
        "outputId": "1fffe52f-8703-43d2-bd9e-1d22be21a101"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.3518 - accuracy: 0.9206\n",
            "Epoch 2/15\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2915 - accuracy: 0.9365\n",
            "Epoch 3/15\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.2543 - accuracy: 0.9841\n",
            "Epoch 4/15\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.2185 - accuracy: 0.9841\n",
            "Epoch 5/15\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.1970 - accuracy: 0.9524\n",
            "Epoch 6/15\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1560 - accuracy: 0.9841\n",
            "Epoch 7/15\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1418 - accuracy: 1.0000\n",
            "Epoch 8/15\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1084 - accuracy: 0.9841\n",
            "Epoch 9/15\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0831 - accuracy: 1.0000\n",
            "Epoch 10/15\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0778 - accuracy: 1.0000\n",
            "Epoch 11/15\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0588 - accuracy: 1.0000\n",
            "Epoch 12/15\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0430 - accuracy: 1.0000\n",
            "Epoch 13/15\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0359 - accuracy: 1.0000\n",
            "Epoch 14/15\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0257 - accuracy: 1.0000\n",
            "Epoch 15/15\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0225 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1d7724dc50>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"test1\"\n",
        "path = os.path.join(train_dir)\n",
        "#os.listdir(path)\n",
        "X_test = []\n",
        "id_line = []\n",
        "def create_test1_data(path):\n",
        "    for p in os.listdir(path):\n",
        "        id_line.append(p.split(\".\")[0])\n",
        "        img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n",
        "        new_img_array = cv2.resize(img_array, dsize=(80, 80))\n",
        "        X_test.append(new_img_array)\n",
        "create_test1_data(path)\n",
        "X_test = np.array(X_test).reshape(-1,80,80,1)\n",
        "X_test = X_test/255\n",
        "predictions = model.predict(X_test)\n",
        "predicted_val = [int(round(p[0])) for p in predictions]\n",
        "submission_df = pd.DataFrame({'id':id_line, 'label':predicted_val})\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n",
        "#是貓回傳1 是狗回傳0"
      ],
      "metadata": {
        "id": "LKsAHf71xEWq"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}